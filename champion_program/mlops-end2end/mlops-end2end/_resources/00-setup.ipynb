{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6212b72-cc13-4de1-9fc5-ea7aee0af5a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install \"databricks-sdk>=0.28.0\" -qU\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f20596f-d044-4180-86de-5e536ba68e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")\n",
    "dbutils.widgets.dropdown(\"gen_synthetic_data\", \"false\", [\"true\", \"false\"], \"Generate Synthetic data for Drift Detection\")\n",
    "dbutils.widgets.dropdown(\"adv_mlops\", \"false\", [\"true\", \"false\"], \"Setup for advanced MLOps demo\")\n",
    "dbutils.widgets.dropdown(\"setup_inference_data\", \"false\", [\"true\", \"false\"], \"Setup inference data for quickstart\")\n",
    "dbutils.widgets.dropdown(\"setup_adv_inference_data\", \"false\", [\"true\", \"false\"], \"Setup inference data for advanced demo\")\n",
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\"\n",
    "setup_inference_data = dbutils.widgets.get(\"setup_inference_data\") == \"true\"\n",
    "setup_adv_inference_data = dbutils.widgets.get(\"setup_adv_inference_data\") == \"true\"\n",
    "generate_synthetic_data = dbutils.widgets.get(\"gen_synthetic_data\") == \"true\"\n",
    "is_advanced_mlops_demo = dbutils.widgets.get(\"adv_mlops\") == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5501f52a-0aed-4881-9087-52895d597d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "reformat_current_user = current_user.split(\"@\")[0].lower().replace(\".\", \"_\")\n",
    "\n",
    "catalog = \"main\"\n",
    "dbName = db = \"dbdemos_mlops\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a52e462-e9bb-4a3f-b24b-6183be4d2eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-global-setup-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2968063d-f063-42d6-8fff-55abe757ee75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "#remove warnings for nicer display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)\n",
    "\n",
    "from mlflow import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f79207a9-ab00-461b-aac0-6490d206d61e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.setup_schema(catalog, db, reset_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "474ecb95-8300-46cd-b4e3-d2ae21dbe420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set UC Model Registry as default\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "if is_advanced_mlops_demo:\n",
    "  # Set Experiment name as default\n",
    "  xp_path = f\"/Users/{current_user}/databricks_automl\"\n",
    "  xp_name = \"advanced_mlops_churn_demo_experiment\"\n",
    "\n",
    "  # Create directory in case it doesn't exist\n",
    "  try: \n",
    "    dbutils.fs.mkdirs(f'file:/Workspace{xp_path}')\n",
    "  except:\n",
    "    import os \n",
    "    os.makedirs(f'/Workspace{xp_path}', exist_ok=True)  \n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60292782-677e-4934-b210-e8d6839e51fd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Raw/Bronze customer data from IBM Telco public dataset and sanitize column name"
    }
   },
   "outputs": [],
   "source": [
    "# Default for quickstart\n",
    "bronze_table_name = \"mlops_churn_bronze_customers\"\n",
    "\n",
    "# Bronze table name for advanced\n",
    "if is_advanced_mlops_demo:\n",
    "  bronze_table_name = \"advanced_churn_bronze_customers\"\n",
    "\n",
    "if reset_all_data or not spark.catalog.tableExists(bronze_table_name):\n",
    "  import requests\n",
    "  from io import StringIO\n",
    "  #Dataset under apache license: https://github.com/IBM/telco-customer-churn-on-icp4d/blob/master/LICENSE\n",
    "  csv = requests.get(\"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\").text\n",
    "  df = pd.read_csv(StringIO(csv), sep=\",\")\n",
    "  def cleanup_column(pdf):\n",
    "    # Clean up column names\n",
    "    pdf.columns = [re.sub(r'(?<!^)(?=[A-Z])', '_', name).lower().replace(\"__\", \"_\") for name in pdf.columns]\n",
    "    pdf.columns = [re.sub(r'[\\(\\)]', '', name).lower() for name in pdf.columns]\n",
    "    pdf.columns = [re.sub(r'[ -]', '_', name).lower() for name in pdf.columns]\n",
    "    return pdf.rename(columns = {'streaming_t_v': 'streaming_tv', 'customer_i_d': 'customer_id'})\n",
    "  \n",
    "  if is_advanced_mlops_demo:\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    w = WorkspaceClient()\n",
    "    try:\n",
    "      print(f\"Deleting existing monitors for {catalog}.{db}.advanced_churn_inference_table\")\n",
    "      w.quality_monitors.delete(table_name=f\"{catalog}.{db}.advanced_churn_inference_table\")\n",
    "    except Exception as error:\n",
    "      print(f\"Error deleting monitor: {type(error).__name__}\")\n",
    "    experiment_details = client.get_experiment_by_name(f\"{xp_path}/{xp_name}\")\n",
    "    if experiment_details:\n",
    "      print(f' Deleting experiment: {experiment_details.experiment_id}')\n",
    "      client.delete_experiment(f'{experiment_details.experiment_id}')\n",
    "  \n",
    "  df = cleanup_column(df)\n",
    "  print(f\"creating `{bronze_table_name}` raw table\")\n",
    "  spark.createDataFrame(df).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(bronze_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4a82d5e-cac5-43e1-95d8-695b7f0f3fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delete_feature_store_table(catalog, db, feature_table_name):\n",
    "  from databricks.feature_engineering import FeatureEngineeringClient\n",
    "  fe = FeatureEngineeringClient()\n",
    "  try:\n",
    "    # Drop existing table from Feature Store\n",
    "    fe.drop_table(name=f\"{catalog}.{db}.{feature_table_name}\")\n",
    "    # Delete underyling delta tables\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{db}.{feature_table_name}\")\n",
    "    print(f\"Dropping Feature Table {catalog}.{db}.{feature_table_name}\")\n",
    "  except ValueError as ve:\n",
    "    print(f\"Feature Table {catalog}.{db}.{feature_table_name} doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd72cc7c-cdf4-4ff2-b1bd-e00455a3b0cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This setup is used in the quickstart demo only\n",
    "\n",
    "quickstart_training_table_name = \"mlops_churn_training\"\n",
    "quickstart_unlabelled_table_name = \"mlops_churn_inference\"\n",
    "\n",
    "if setup_inference_data:\n",
    "  # Check that the training table exists first, as we'll be creating a copy of it\n",
    "  if spark.catalog.tableExists(f\"{catalog}.{db}.{quickstart_training_table_name}\"):\n",
    "    # This should only be called from the quickstart challenger validation or batch inference notebooks\n",
    "    if not spark.catalog.tableExists(f\"{catalog}.{db}.{quickstart_unlabelled_table_name}\"):\n",
    "      print(\"Creating unlabelled data table for performing inference...\")\n",
    "      # Drop the label column for inference\n",
    "      spark.read.table(quickstart_training_table_name).drop(\"churn\").write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(quickstart_unlabelled_table_name)\n",
    "  else:\n",
    "    print(\"Training table doesn't exist, please run the notebook '01_feature_engineering'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ef08aa3-0e2a-47f0-8dfe-11a8ac3fa0d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This setup is used in the advanced demo only\n",
    "#advanced_label_table_name = \"churn_label_table\"\n",
    "#advanced_unlabelled_table_name = \"mlops_churn_advanced_cust_ids\"\n",
    "\n",
    "if setup_adv_inference_data:\n",
    "  # Check that the label table exists first, as we'll be creating a copy of it\n",
    "  if spark.catalog.tableExists(f\"advanced_churn_label_table\"):\n",
    "    # This should only be called from the advanced batch inference notebook\n",
    "    # if not spark.catalog.tableExists(f\"advanced_churn_cust_ids\"):\n",
    "    print(\"Creating table with customer records for inference...\")\n",
    "    # Drop the label column for inference\n",
    "    # This seems to be writing to the wrong table. Comment out first to test writing to advanced_churn_cust_ids.\n",
    "    #spark.read.table(\"advanced_churn_label_table\").drop(\"churn\",\"split\").write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"churn_label_table\")\n",
    "    spark.read.table(\"advanced_churn_label_table\").drop(\"churn\",\"split\").write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"advanced_churn_cust_ids\")\n",
    "  else:\n",
    "    print(\"Label table `advanced_churn_label_table` doesn't exist, please run the notebook '01_feature_engineering'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8c7ad73-a1ba-4f2b-9fbc-09f7cbd9ffa1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate inference synthetic data"
    }
   },
   "outputs": [],
   "source": [
    "gen_synthetic_data = False\n",
    "def generate_synthetic(inference_table, drift_type=\"label_drift\"):\n",
    "  import dbldatagen as dg\n",
    "  import pyspark.sql.types\n",
    "  from databricks.feature_engineering import FeatureEngineeringClient\n",
    "  import pyspark.sql.functions as F\n",
    "  from datetime import datetime, timedelta\n",
    "  # Column definitions are stubs only - modify to generate correct data  \n",
    "  #\n",
    "  generation_spec = (\n",
    "      dg.DataGenerator(sparkSession=spark, \n",
    "                      name='synthetic_data', \n",
    "                      rows=5000,\n",
    "                      random=True,\n",
    "                      )\n",
    "      .withColumn('customer_id', 'string', template=r'dddd-AAAA')\n",
    "      .withColumn('transaction_ts', 'timestamp', begin=(datetime.now() + timedelta(days=-30)), end=(datetime.now() + timedelta(days=-1)), interval=\"1 hour\")\n",
    "      .withColumn('gender', 'string', values=['Female', 'Male'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('senior_citizen', 'string', values=['No', 'Yes'], random=True, weights=[0.85, 0.15])\n",
    "      .withColumn('partner', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('dependents', 'string', values=['No', 'Yes'], random=True, weights=[0.7, 0.3])\n",
    "      .withColumn('tenure', 'double', minValue=0.0, maxValue=72.0, step=1.0)\n",
    "      .withColumn('phone_service', values=['No', 'Yes'], random=True, weights=[0.9, 0.1])\n",
    "      .withColumn('multiple_lines', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('internet_service', 'string', values=['Fiber optic', 'DSL', 'No'], random=True, weights=[0.5, 0.3, 0.2])\n",
    "      .withColumn('online_security', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('online_backup', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('device_protection', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('tech_support', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('streaming_tv', 'string', values=['No', 'Yes', 'No internet service'], random=True, weights=[0.4, 0.4, 0.2])\n",
    "      .withColumn('streaming_movies', 'string', values=['No', 'Yes', 'No internet service'], random=True, weights=[0.4, 0.4, 0.2])\n",
    "      .withColumn('contract', 'string', values=['Month-to-month', 'One year','Two year'], random=True, weights=[0.5, 0.25, 0.25])\n",
    "      .withColumn('paperless_billing', 'string', values=['No', 'Yes'], random=True, weights=[0.6, 0.4])\n",
    "      .withColumn('payment_method', 'string', values=['Credit card (automatic)', 'Mailed check',\n",
    "  'Bank transfer (automatic)', 'Electronic check'], weights=[0.2, 0.2, 0.2, 0.4])\n",
    "      .withColumn('monthly_charges', 'double', minValue=18.0, maxValue=118.0, step=0.5)\n",
    "      .withColumn('total_charges', 'double', minValue=0.0, maxValue=8684.0, step=20)\n",
    "      .withColumn('num_optional_services', 'double', minValue=0.0, maxValue=6.0, step=1)\n",
    "      .withColumn('avg_price_increase', 'float', minValue=-19.0, maxValue=130.0, step=20)\n",
    "      .withColumn('churn', 'string', values=['Yes'], random=True)\n",
    "      )\n",
    "\n",
    "\n",
    "  # Generate Synthetic Data\n",
    "  df_synthetic_data = generation_spec.build()\n",
    "\n",
    "  fe = FeatureEngineeringClient()\n",
    "\n",
    "  # Model URI\n",
    "  model_uri = f\"models:/{model_name}@Champion\"\n",
    "\n",
    "  # Batch score\n",
    "  preds_df = fe.score_batch(df=df_synthetic_data, model_uri=model_uri, result_type=\"string\")\n",
    "  preds_df = preds_df \\\n",
    "    .withColumn('model_name', F.lit(f\"{model_name}\")) \\\n",
    "    .withColumn('model_version', F.lit(1)) \\\n",
    "    .withColumn('model_alias', F.lit(\"Champion\")) \\\n",
    "    .withColumn('inference_timestamp', F.lit(datetime.now()- timedelta(days=1))) \n",
    "\n",
    "  preds_df.write.mode(\"append\").saveAsTable(f\"{catalog}.{db}.{inference_table_name}\")\n",
    "\n",
    "if is_advanced_mlops_demo:\n",
    "  model_name = f\"{catalog}.{db}.advanced_mlops_churn\"\n",
    "  inference_table_name = \"advanced_churn_inference_table\"\n",
    "  if generate_synthetic_data:\n",
    "    generate_synthetic(inference_table=inference_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6f913cd-4109-4192-bd15-d45645d82826",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get slack webhook"
    }
   },
   "outputs": [],
   "source": [
    "# Replace this with your Slack webhook\n",
    "try:\n",
    "  slack_webhook = dbutils.secrets.get(scope=\"fieldeng\", key=f\"{user_name}_slack_webhook\")\n",
    "except:\n",
    "  slack_webhook = \"\" # https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0541234f-ad76-4467-8a33-f75e1455ec2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "#from databricks.feature_store import FeatureStoreClient\n",
    "import mlflow\n",
    "import databricks\n",
    "from datetime import datetime\n",
    "\n",
    "def get_automl_run(name):\n",
    "  #get the most recent automl run\n",
    "  df = spark.table(\"field_demos_metadata.automl_experiment\").filter(col(\"name\") == name).orderBy(col(\"date\").desc()).limit(1)\n",
    "  return df.collect()\n",
    "\n",
    "#Get the automl run information from the field_demos_metadata.automl_experiment table. \n",
    "#If it's not available in the metadata table, start a new run with the given parameters\n",
    "def get_automl_run_or_start(name, model_name, dataset, target_col, timeout_minutes):\n",
    "  spark.sql(\"create database if not exists field_demos_metadata\")\n",
    "  spark.sql(\"create table if not exists field_demos_metadata.automl_experiment (name string, date string)\")\n",
    "  result = get_automl_run(name)\n",
    "  if len(result) == 0:\n",
    "    print(\"No run available, start a new Auto ML run, this will take a few minutes...\")\n",
    "    start_automl_run(name, model_name, dataset, target_col, timeout_minutes)\n",
    "    result = get_automl_run(name)\n",
    "  return result[0]\n",
    "\n",
    "\n",
    "#Start a new auto ml classification task and save it as metadata.\n",
    "def start_automl_run(name, model_name, dataset, target_col, timeout_minutes = 5):\n",
    "  automl_run = databricks.automl.classify(\n",
    "    dataset = dataset,\n",
    "    target_col = target_col,\n",
    "    timeout_minutes = timeout_minutes\n",
    "  )\n",
    "  experiment_id = automl_run.experiment.experiment_id\n",
    "  path = automl_run.experiment.name\n",
    "  data_run_id = mlflow.search_runs(experiment_ids=[automl_run.experiment.experiment_id], filter_string = \"tags.mlflow.source.name='Notebook: DataExploration'\").iloc[0].run_id\n",
    "  exploration_notebook_id = automl_run.experiment.tags[\"_databricks_automl.exploration_notebook_id\"]\n",
    "  best_trial_notebook_id = automl_run.experiment.tags[\"_databricks_automl.best_trial_notebook_id\"]\n",
    "\n",
    "  cols = [\"name\", \"date\", \"experiment_id\", \"experiment_path\", \"data_run_id\", \"best_trial_run_id\", \"exploration_notebook_id\", \"best_trial_notebook_id\"]\n",
    "  spark.createDataFrame(data=[(name, datetime.today().isoformat(), experiment_id, path, data_run_id, automl_run.best_trial.mlflow_run_id, exploration_notebook_id, best_trial_notebook_id)], schema = cols).write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"field_demos_metadata.automl_experiment\")\n",
    "  #Create & save the first model version in the MLFlow repo (required to setup hooks etc)\n",
    "  mlflow.register_model(f\"runs:/{automl_run.best_trial.mlflow_run_id}/model\", model_name)\n",
    "  return get_automl_run(name)\n",
    "\n",
    "#Generate nice link for the given auto ml run\n",
    "def display_automl_link(name, model_name, dataset, target_col, force_refresh=False, timeout_minutes = 5):\n",
    "  r = get_automl_run_or_start(name, model_name, dataset, target_col, timeout_minutes)\n",
    "  html = f\"\"\"For exploratory data analysis, open the <a href=\"/#notebook/{r[\"exploration_notebook_id\"]}\">data exploration notebook</a><br/><br/>\"\"\"\n",
    "  html += f\"\"\"To view the best performing model, open the <a href=\"/#notebook/{r[\"best_trial_notebook_id\"]}\">best trial notebook</a><br/><br/>\"\"\"\n",
    "  html += f\"\"\"To view details about all trials, navigate to the <a href=\"/#mlflow/experiments/{r[\"experiment_id\"]}/s?orderByKey=metrics.%60val_f1_score%60&orderByAsc=false\">MLflow experiment</>\"\"\"\n",
    "  displayHTML(html)\n",
    "\n",
    "\n",
    "def display_automl_churn_link(): \n",
    "  display_automl_link(\"churn_auto_ml\", \"field_demos_customer_churn\", spark.table(\"churn_features\"), \"churn\", 5)\n",
    "\n",
    "def get_automl_churn_run(): \n",
    "  return get_automl_run_or_start(\"churn_auto_ml\", \"field_demos_customer_churn\", spark.table(\"churn_features\"), \"churn\", 5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
