{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf4187ab-b9a7-46bd-ac8b-9cb82ce2f77a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Drift detection\n",
    "\n",
    "In this step, we will define drift detection rules to run periodically on the inference data.\n",
    "\n",
    "**Drift detection** refers to the process of identifying changes in the statistical properties of input data, which can lead to a decline in model performance over time. This is crucial for maintaining the accuracy and reliability of models in dynamic environments, as it allows for timely interventions such as model retraining or adaptation to new data distributions\n",
    "\n",
    "In order to simulate some data drifts, we will use [_dbldatagen_ library](https://github.com/databrickslabs/dbldatagen), a Databricks Labs project which is a Python library for generating synthetic data using Spark.\n",
    "\n",
    "We will simulate label drift using the data generator package.\n",
    "**Label drift** occurs when the distribution of the ground truth labels changes over time, which can happen due to shifts in labeling criteria or the introduction of labeling errors.\n",
    "\n",
    "_We will set all labels to True_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92f33f5b-c490-4209-bc6c-037e3fce07e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/advanced/banners/mlflow-uc-end-to-end-advanced-8.png?raw=true\" width=\"1200\">\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=3227024006299960&notebook=%2F02-mlops-advanced%2F08_drift_detection&demo_name=mlops-end2end&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fmlops-end2end%2F02-mlops-advanced%2F08_drift_detection&version=1\">\n",
    "<!-- [metadata={\"description\":\"MLOps end2end workflow: Batch to automatically retrain model on a monthly basis.\",\n",
    " \"authors\":[\"quentin.ambard@databricks.com\"],\n",
    " \"db_resources\":{},\n",
    "  \"search_tags\":{\"vertical\": \"retail\", \"step\": \"Model testing\", \"components\": [\"mlflow\"]},\n",
    "                 \"canonicalUrl\": {\"AWS\": \"\", \"Azure\": \"\", \"GCP\": \"\"}}] -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6933dccd-2636-4b1c-bf1b-085160d63fd0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install needed package"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU databricks-sdk==0.40.0 mlflow==2.19 databricks-feature-engineering==0.8.0\n",
    "%pip install -qU dbldatagen\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c038a067-c3a7-48eb-8496-0c991a2e56f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define drift metrics"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"perf_metric\", \"f1_score.macro\", [\"accuracy_score\", \"precision.weighted\", \"recall.weighted\", \"f1_score.macro\"])\n",
    "dbutils.widgets.dropdown(\"drift_metric\", \"js_distance\", [\"chi_squared_test.statistic\", \"chi_squared_test.pvalue\", \"tv_distance\", \"l_infinity_distance\", \"js_distance\"])\n",
    "dbutils.widgets.text(\"model_id\", \"*\", \"Model Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e068080-71b4-4c86-a64a-4fea3c78cc72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run setup notebook & generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4af8f052-1f6c-459f-8df1-9a3bdd628fda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false $adv_mlops=true $gen_synthetic_data=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "246a6bf8-efdb-45e7-9df1-ec575696f1ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Refresh the monitor \n",
    "\n",
    "The previous step performs a write of the synthetic data to the inteference table. We should referesh the monitor to re-compute the metrics.\n",
    "\n",
    "**PS:** Refresh is only necessary if the monitored table has undergone changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ea4fc49-33b6-49b1-917d-e15dbef42822",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Refresh the monitor"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import MonitorInfoStatus, MonitorRefreshInfoState\n",
    "\n",
    "\n",
    "w = WorkspaceClient()\n",
    "refresh_info = w.quality_monitors.run_refresh(table_name=f\"{catalog}.{db}.advanced_churn_inference_table\")\n",
    "\n",
    "while refresh_info.state in (MonitorRefreshInfoState.PENDING, MonitorRefreshInfoState.RUNNING):\n",
    "  refresh_info = w.quality_monitors.get_refresh(table_name=f\"{catalog}.{db}.advanced_churn_inference_table\", refresh_id=refresh_info.refresh_id)\n",
    "  time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5758948-8785-4bdd-8fa6-45819c74adb1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Programmatically retrieve profile and drift table names from monitor info"
    }
   },
   "outputs": [],
   "source": [
    "monitor_info = w.quality_monitors.get(table_name=f\"{catalog}.{db}.advanced_churn_inference_table\")\n",
    "drift_table_name = monitor_info.drift_metrics_table_name\n",
    "profile_table_name = monitor_info.profile_metrics_table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05870c5c-669e-427c-a478-43aa2ddf89ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Inspect dashboard\n",
    "\n",
    "Once the monitor is refreshed, refreshing the monitoring dashboard will show the latest model performance metrics. When evaluated against the latest labelled data, the model has poor accuracy, weighted F1 score and recall. On the other hand, it has a weighted precision of 1.\n",
    "\n",
    "We expect this because the model is now heavily weighted towards the `churn = Yes` class. All predictions of `Yes` are correct, leading to a weighted precision of 1.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/advanced/08_model_kpis.png?raw=true\" width=\"1200\">\n",
    "\n",
    "<br>\n",
    "\n",
    "We will go ahead and illustrate how you can programatically retrieve the drift metrics and trigger model retraining.\n",
    "\n",
    "However, it is worthwhile to mention that by inspecting the confusion matrix in the monitoring dashboard, we can see that the latest labelled data only has the `Yes` label. i.e. all customers have churned. This is an unlikely scenario. That should lead us to question whether labelling was done correctly, or if there were data quality issues upstream. These causes of label drift do not necessitate model retraining.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/advanced/08_confusion_matrix.png?raw=true\" width=\"1200\">\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fb1864a-22ad-4d5b-b880-b7f42704488a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Retrieve drift metrics\n",
    "\n",
    "Query Lakehouse Monitoring's drift metrics table for the inference table being monitored.\n",
    "Here we're testing if these metrics have exceeded a certain threshold (defined by the business):\n",
    "1. Prediction drift (Jensen–Shannon distance) > 0.2\n",
    "2. Label drift (Jensen–Shannon distance) > 0.2\n",
    "3. Expected Loss (daily average per user) > 30\n",
    "4. Performance(i.e. F1-Score) < 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41864cc8-8791-429c-8d20-9c575375592e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metric = dbutils.widgets.get(\"perf_metric\")\n",
    "drift = dbutils.widgets.get(\"drift_metric\")\n",
    "model_id = dbutils.widgets.get(\"model_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "741c065b-a365-450e-b94a-fbbcac16499f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Construct dataframe to detect performance degradation from the profile metrics table generated by lakehouse monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b10c75f6-b990-4afc-b3cb-68ed7235d2ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dataframe for performance metrics"
    }
   },
   "outputs": [],
   "source": [
    "performance_metrics_df = spark.sql(f\"\"\"\n",
    "SELECT\n",
    "  window.start as time,\n",
    "  {metric} AS performance_metric,\n",
    "  expected_loss,\n",
    "  Model_Version AS `Model Id`\n",
    "FROM {profile_table_name}\n",
    "WHERE\n",
    "  window.start >= \"2024-06-01\"\n",
    "\tAND log_type = \"INPUT\"\n",
    "  AND column_name = \":table\"\n",
    "  AND slice_key is null\n",
    "  AND slice_value is null\n",
    "  AND Model_Version = '{model_id}'\n",
    "ORDER BY\n",
    "  window.start\n",
    "\"\"\"\n",
    ")\n",
    "display(performance_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50961fa2-1417-4e11-a7bc-af32e94144e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Construct dataframe to detect drifts from the drift metrics table generated by lakehouse monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7601e33-9303-4ceb-800b-5d8c293cdc69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drift_metrics_df = spark.sql(f\"\"\"\n",
    "  SELECT\n",
    "  window.start AS time,\n",
    "  column_name,\n",
    "  {drift} AS drift_metric,\n",
    "  Model_Version AS `Model Id`\n",
    "FROM {drift_table_name}\n",
    "WHERE\n",
    "  column_name IN ('prediction', 'churn')\n",
    "  AND window.start >= \"2024-06-01\"\n",
    "  AND slice_key is null\n",
    "  AND slice_value is null\n",
    "  AND Model_Version = '{model_id}'\n",
    "  AND drift_type = \"CONSECUTIVE\"\n",
    "ORDER BY\n",
    "  window.start\n",
    "\"\"\"\n",
    ")\n",
    "display(drift_metrics_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6029d4c1-fac3-4900-adc6-63c7721eeec9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Unstack dataframe"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first\n",
    "# If no drift on the label or prediction, we skip it\n",
    "if not drift_metrics_df.isEmpty():\n",
    "    unstacked_drift_metrics_df = (\n",
    "        drift_metrics_df.groupBy(\"time\", \"`Model Id`\")\n",
    "        .pivot(\"column_name\")\n",
    "        .agg(first(\"drift_metric\"))\n",
    "        .orderBy(\"time\")\n",
    "    )\n",
    "    display(unstacked_drift_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cb6905b-4063-4241-834d-aee273da7c4f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Join all metrics together"
    }
   },
   "outputs": [],
   "source": [
    "all_metrics_df = performance_metrics_df\n",
    "if not drift_metrics_df.isEmpty():\n",
    "    all_metrics_df = performance_metrics_df.join(\n",
    "        unstacked_drift_metrics_df, on=[\"time\", \"Model Id\"], how=\"inner\"\n",
    "    )\n",
    "\n",
    "display(all_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c59fa541-e6d4-4fb0-b5e3-59f2acda9e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Count total violations and save as task value\n",
    "\n",
    "Here we will define the different threshholds for the metrics we are interested in to qualify a drift:\n",
    "- Performance metric < 0.5 \n",
    "- Average Expected Loss per customer (our custom metric connected to business) > 30 dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2552f0a0-4032-4499-8424-d758a301867b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "count nr violations"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, abs\n",
    "\n",
    "\n",
    "performance_violation_count = all_metrics_df.where(\n",
    "    (col(\"performance_metric\") < 0.5) & (abs(col(\"expected_loss\")) > 30)\n",
    ").count()\n",
    "\n",
    "drift_violation_count = 0\n",
    "if not drift_metrics_df.isEmpty():\n",
    "    drift_violation_count = all_metrics_df.where(\n",
    "        (col(\"churn\") > 0.19) & (col(\"prediction\") > 0.19)\n",
    "    ).count()\n",
    "\n",
    "all_violations_count = drift_violation_count + performance_violation_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85978385-1d96-4600-9c4c-2891ce37ab5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total number of joint violations: {all_violations_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9150744d-e285-42ea-abfb-d98d9655800b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next: Trigger model retraining\n",
    "\n",
    "Upon detecting the number of violations, we should automate some actions, such as:\n",
    "- Retrain the machine learning model\n",
    "- Send an alert to owners via slack or email\n",
    "\n",
    "One way of performing this in Databricks is to add branching logic to your job with [the If/else condition task](https://docs.databricks.com/en/jobs/conditional-tasks.html#add-branching-logic-to-your-job-with-the-ifelse-condition-task). \n",
    "\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/product/mlops/advanced/08_view_retraining_workflow.png?raw=true\" width=\"1200\">\n",
    "\n",
    "In order to do that, we should save the number of violations in a [task value](https://docs.databricks.com/en/jobs/share-task-context.html) to be consumed in the If/else condition. \n",
    "\n",
    "In our workflow, we will trigger a model training, which will be a job run task of the train model job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe61491c-ce25-4fd8-8d54-1f8130148b5e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exit notebook by setting a task value"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.jobs.taskValues.set(key = 'all_violations_count', value = all_violations_count)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "08_drift_detection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
