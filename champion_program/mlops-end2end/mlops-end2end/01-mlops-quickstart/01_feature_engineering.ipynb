{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e663937-2abb-4265-86ee-3b82dc0e4143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Churn Prediction Feature Engineering\n",
    "Our first step is to analyze the data and build the features we'll use to train our model. Let's see how this can be done.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/mlops-uc-end2end-1.png?raw=true\" width=\"1200\">\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=3227024006299960&notebook=%2F01-mlops-quickstart%2F01_feature_engineering&demo_name=mlops-end2end&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fmlops-end2end%2F01-mlops-quickstart%2F01_feature_engineering&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8095570e-1154-499e-bae9-992b5278a758",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install latest feature engineering client for UC [for MLR < 13.2] and databricks python sdk"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet mlflow==2.19\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e236892-3482-4f0b-a2a9-4914240da7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d529a56-8505-4104-9a68-b1db93f57916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Anaylsis\n",
    "To get a feel of the data, what needs cleaning, pre-processing etc.\n",
    "- **Use Databricks's native visualization tools**\n",
    "  - After running a SQL query in a notebook cell, use the `+` tab to add charts to visualize the results.\n",
    "- Bring your own visualization library of choice (i.e. seaborn, plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c596637c-afd0-44f2-8d01-8387510b9116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM mlops_churn_bronze_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fba1c006-61db-4023-b1ec-e94708675307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_df = spark.read.table(\"mlops_churn_bronze_customers\").pandas_api()\n",
    "telco_df[\"internet_service\"].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5a099b0-96de-440f-8cc6-ce7342c61dbc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read in Bronze Delta table using Spark"
    }
   },
   "outputs": [],
   "source": [
    "# Read into Spark\n",
    "telcoDF = spark.read.table(\"mlops_churn_bronze_customers\")\n",
    "display(telcoDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83810f0c-8d22-4dbe-a861-b69dfb505d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define data cleaning and featurization Logic\n",
    "\n",
    "We will define a function to clean the data and implement featurization logic. We will:\n",
    "\n",
    "1. Compute number of optional services\n",
    "2. Provide meaningful labels\n",
    "3. Impute null values\n",
    "\n",
    "_This can also work for streaming based features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "600f500d-6cf1-40e7-b1fc-b3c015567aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using Pandas On Spark API\n",
    "\n",
    "Because our Data Scientist team is familiar with Pandas, we'll use the [pandas on spark API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html) to scale `pandas` code. The Pandas instructions will be converted in the spark engine under the hood and distributed at scale.\n",
    "\n",
    "*Note: Pandas API on Spark used to be called Koalas. Starting from `spark 3.2`, Koalas is builtin and we can get an Pandas Dataframe using `pandas_api()` [Details](https://spark.apache.org/docs/latest/api/python/migration_guide/koalas_to_pyspark.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75629317-d837-4d00-8e26-8e042e9399a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define featurization function"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def clean_churn_features(dataDF: DataFrame) -> DataFrame:\n",
    "  \"\"\"\n",
    "  Simple cleaning function leveraging pandas API\n",
    "  \"\"\"\n",
    "\n",
    "  # Convert to pandas on spark dataframe\n",
    "  data_psdf = dataDF.pandas_api()\n",
    "  # Convert some columns\n",
    "  data_psdf = data_psdf.astype({\"senior_citizen\": \"string\"})\n",
    "  data_psdf[\"senior_citizen\"] = data_psdf[\"senior_citizen\"].map({\"1\" : \"Yes\", \"0\" : \"No\"})\n",
    "\n",
    "  data_psdf[\"total_charges\"] = data_psdf[\"total_charges\"].apply(lambda x: float(x) if x.strip() else 0)\n",
    "\n",
    "\n",
    "  # Fill some missing numerical values with 0\n",
    "  data_psdf = data_psdf.fillna({\"tenure\": 0.0})\n",
    "  data_psdf = data_psdf.fillna({\"monthly_charges\": 0.0})\n",
    "  data_psdf = data_psdf.fillna({\"total_charges\": 0.0})\n",
    "\n",
    "  def sum_optional_services(df):\n",
    "      \"\"\"Count number of optional services enabled, like streaming TV\"\"\"\n",
    "      cols = [\"online_security\", \"online_backup\", \"device_protection\", \"tech_support\",\n",
    "              \"streaming_tv\", \"streaming_movies\"]\n",
    "      return sum(map(lambda c: (df[c] == \"Yes\"), cols))\n",
    "\n",
    "  data_psdf[\"num_optional_services\"] = sum_optional_services(data_psdf)\n",
    "\n",
    "  # Return the cleaned Spark dataframe\n",
    "  return data_psdf.to_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e27ebf82-8d86-480b-80aa-1449ff4702db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Compute features & write table with features and labels\n",
    "\n",
    "Once our features are ready, we'll save them along with the labels as a Delta Lake table. This can then be retrieved later for model training.\n",
    "\n",
    "In this Quickstart demo, we will look at how we train a model using this labeled dataset saved as a Delta Lake table and capture the table-model lineage. Model lineage brings traceability and governance in our deployment, letting us know which model is dependent of which set of feature tables.\n",
    "\n",
    "Databricks has a Feature Store capability that is tightly integrated into the platform. Any Delta Lake table with a primary key can be used as a Feature Store table for model training, as well as batch and online serving. We will look at an example of how to use the Feature Store to perform feature lookups in a more advanced demo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29ac41e6-4b90-4dca-be94-2345783206d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute Churn Features and append a timestamp"
    }
   },
   "outputs": [],
   "source": [
    "churn_features = clean_churn_features(telcoDF)\n",
    "display(churn_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "767b73de-1231-4492-917c-674e7bc56162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write table for training\n",
    "\n",
    "Write the labeled data that has the prepared features and labels as a Delta Table. We will later use this table to train the model to predict churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feb5b0ef-1985-4180-a804-04fa06dcb4d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify train-val-test split\n",
    "train_ratio, val_ratio, test_ratio = 0.7, 0.2, 0.1\n",
    "churn_features = (churn_features.withColumn(\"random\", F.rand(seed=42))\n",
    "                                .withColumn(\"split\",\n",
    "                                            F.when(F.col(\"random\") < train_ratio, \"train\")\n",
    "                                            .when(F.col(\"random\") < train_ratio + val_ratio, \"validate\")\n",
    "                                            .otherwise(\"test\"))\n",
    "                                .drop(\"random\"))\n",
    "\n",
    "# Write table for training\n",
    "(churn_features.write.mode(\"overwrite\")\n",
    "               .option(\"overwriteSchema\", \"true\")\n",
    "               .saveAsTable(\"mlops_churn_training\"))\n",
    "\n",
    "# Add comment to the table\n",
    "spark.sql(f\"\"\"COMMENT ON TABLE {catalog}.{db}.mlops_churn_training IS \\'The features in this table are derived from the mlops_churn_bronze_customers table in the lakehouse. \n",
    "              We created service features, cleaned up their names.  No aggregations were performed.'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d10a4cc8-bc45-4848-b4ea-056094a0cb26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "That's it! The labeled features are now ready to be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "633f2254-6cbb-4907-98d5-35dbd386afe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Accelerating Churn model creation using Databricks AutoML\n",
    "### A glass-box solution that empowers data teams without taking away control\n",
    "\n",
    "Databricks simplify model creation and MLOps. However, bootstraping new ML projects can still be long and inefficient.\n",
    "\n",
    "Instead of creating the same boilerplate for each new project, Databricks AutoML can automatically generate state of the art models for Classifications, regression, and forecast.\n",
    "\n",
    "\n",
    "<img width=\"1000\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/auto-ml-full.png\"/>\n",
    "\n",
    "<img style=\"float: right\" width=\"600\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/churn-auto-ml.png\"/>\n",
    "\n",
    "Models can be directly deployed, or instead leverage generated notebooks to boostrap projects with best-practices, saving you weeks of efforts.\n",
    "\n",
    "### Using Databricks AutoML with our Churn dataset\n",
    "\n",
    "AutoML is available in the \"Machine Learning\" space. All we have to do is start a new AutoML experiment and select the table we just created (`dbdemos.schema.mlops_churn_training`).\n",
    "\n",
    "Our prediction target is the `churn` column.\n",
    "\n",
    "Click on Start, and Databricks will do the rest.\n",
    "\n",
    "While this is done using the UI, you can also leverage the [python API](https://docs.databricks.com/en/machine-learning/automl/train-ml-model-automl-api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2782ecf3-027f-4b13-b518-016622095ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Using AutoML with labelled feature tables\n",
    "\n",
    "[AutoML](https://docs.databricks.com/en/machine-learning/automl/how-automl-works.html) works on an input table with prepared features and the corresponding labels. For this quicktstart demo, this is what we will be doing. We run AutoML on the table `dbdemos.schema.mlops_churn_training` and capture the table lineage at training time.\n",
    "\n",
    "#### Using AutoML with tables in the Feature Store\n",
    "\n",
    "AutoML also works with tables containing only the ground-truth labels, and joining it with feature tables in the Feature Store. This will be illustrated in a more advanced demo.\n",
    "\n",
    "You can join/use features directly from the Feature Store from the [UI](https://docs.databricks.com/machine-learning/automl/train-ml-model-automl-ui.html#use-existing-feature-tables-from-databricks-feature-store) or [python API](https://docs.databricks.com/en/machine-learning/automl/train-ml-model-automl-api.html#automl-experiment-with-feature-store-example-notebook)\n",
    "* Select the table containing the ground-truth labels\n",
    "* Join remaining features from the feature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e9a129a-2343-4398-91fa-2fbd7b05e859",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run 'baseline' autoML experiment in the back-ground"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "xp_path = \"/Shared/dbdemos/experiments/mlops\"\n",
    "xp_name = f\"automl_churn_{datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}\"\n",
    "\n",
    "churn_features = churn_features.withMetadata(\"num_optional_services\", {\"spark.contentAnnotation.semanticType\":\"numeric\"})\n",
    "try: \n",
    "    from databricks import automl \n",
    "\n",
    "    # Add/Force semantic data types for specific colums (to facilitate autoML and make sure it doesn't interpret it as categorical)\n",
    "\n",
    "    automl_run = automl.classify(\n",
    "        experiment_name = xp_name,\n",
    "        experiment_dir = xp_path,\n",
    "        dataset = churn_features,\n",
    "        target_col = \"churn\",\n",
    "        split_col = \"split\", #This required DBRML 15.3+\n",
    "        timeout_minutes = 10,\n",
    "        exclude_cols ='customer_id'\n",
    "    )\n",
    "    #Make sure all users can access dbdemos shared experiment\n",
    "    DBDemos.set_experiment_permission(f\"{xp_path}/{xp_name}\")\n",
    "\n",
    "except Exception as e: \n",
    "    if \"cannot import name 'automl'\" in str(e):\n",
    "        # Note: cannot import name 'automl' likely means you're using serverless. Dbdemos doesn't support autoML serverless API - this will be improved soon.\n",
    "        # adding a temporary workaround to make sure this works well for now -- ignore this for classic run\n",
    "        DBDemos.create_mockup_automl_run(f\"{xp_path}/{xp_name}\", churn_features.toPandas()) \n",
    "    else: \n",
    "        raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ded5696-f981-4f99-bb9e-de8de2a6f4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using the generated notebook to build our model\n",
    "\n",
    "Next step: [Explore the generated Auto-ML notebook]($./02_automl_best_run)\n",
    "\n",
    "**Note:**\n",
    "For demo purposes, run the above notebook to create and register a new version of the model from your autoML experiment and label/alias the model as \"Champion\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
