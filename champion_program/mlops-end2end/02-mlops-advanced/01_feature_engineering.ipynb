{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff4895cd-5862-44f8-81d0-da965a6d9581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.widgets.dropdown(\"force_refresh_automl\", \"true\", [\"false\", \"true\"], \"Restart AutoML run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40d90059-7cb4-4f2b-ae9e-617c0b2bf944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": null
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `Current Cluster` from the dropdown menu ([open cluster configuration](https://enb-deloitte.cloud.databricks.com/#setting/clusters/0106-134225-g5g75oj5/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('mlops-end2end')` or re-install the demo: `dbdemos.install('mlops-end2end')`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71908aa0-77a9-43c7-8c47-43223b8268ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Churn Prediction Feature Engineering\n",
    "Our first step is to analyze the data and build the features we'll use to train our model. Let's see how this can be done.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/advanced/banners/mlflow-uc-end-to-end-advanced-1.png?raw=true\" width=\"1200\">\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=3227024006299960&notebook=%2F02-mlops-advanced%2F01_feature_engineering&demo_name=mlops-end2end&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fmlops-end2end%2F02-mlops-advanced%2F01_feature_engineering&version=1\">\n",
    "<!-- [metadata={\"description\":\"MLOps end2end workflow: Feature engineering\",\n",
    " \"authors\":[\"quentin.ambard@databricks.com\"],\n",
    " \"db_resources\":{},\n",
    "  \"search_tags\":{\"vertical\": \"retail\", \"step\": \"Data Engineering\", \"components\": [\"feature store\"]},\n",
    "                 \"canonicalUrl\": {\"AWS\": \"\", \"Azure\": \"\", \"GCP\": \"\"}}] -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d82e16a3-5ba6-46da-9e52-ebb6f2290151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet mlflow==2.19 databricks-feature-engineering==0.8.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "815d20d8-330c-4f80-a0bf-bf5f627dd72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false $adv_mlops=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a4d37fe-639a-4762-b0ed-89560c44d46a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Anaylsis\n",
    "To get a feel of the data, what needs cleaning, pre-processing etc.\n",
    "- **Use Databricks's native visualization tools**\n",
    "- Bring your own visualization library of choice (i.e. seaborn, plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ad53fdf-3e80-44de-a751-b7b557f2759d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read in Bronze Delta table using Spark"
    }
   },
   "outputs": [],
   "source": [
    "# Read into spark dataframe\n",
    "telcoDF = spark.read.table(\"advanced_churn_bronze_customers\")\n",
    "display(telcoDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32f359d1-2ecd-4355-bb0d-b0488f58c9d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Featurization Logic(s) for BATCH feature computation\n",
    "\n",
    "1. Compute number of active services\n",
    "2. Clean-up names and manual mapping\n",
    "\n",
    "_This can also work for streaming based features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "702813c3-999d-44cb-b2a7-d5b538202c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using PandasUDF and PySpark\n",
    "To scale pandas analytics on a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbbefcc2-48c7-444e-b5ca-8ce9477f8d7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql.functions import pandas_udf, col, when, lit\n",
    "\n",
    "\n",
    "#  Count number of optional services enabled, like streaming TV\n",
    "def compute_service_features(inputDF: SparkDataFrame) -> SparkDataFrame:\n",
    "  # Create pandas UDF function\n",
    "  @pandas_udf('double')\n",
    "  def num_optional_services(*cols):\n",
    "    # Nested helper function to count number of optional services in a pandas dataframe\n",
    "    return sum(map(lambda s: (s == \"Yes\").astype('double'), cols))\n",
    "\n",
    "  return inputDF.\\\n",
    "    withColumn(\"num_optional_services\",\n",
    "        num_optional_services(\"online_security\", \"online_backup\", \"device_protection\", \"tech_support\", \"streaming_tv\", \"streaming_movies\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ab31ef8-81a0-4811-b6e8-145823674288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using Pandas On Spark API\n",
    "\n",
    "Because our Data Scientist team is familiar with Pandas, we'll use the [pandas on spark API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html) to scale `pandas` code. The Pandas instructions will be converted in the spark engine under the hood and distributed at scale.\n",
    "\n",
    "*Note: Starting from `spark 3.2`, koalas is builtin and we can get an Pandas Dataframe using `pandas_api()`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "481ed0fc-1247-43b1-a7f6-06822a83a23b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define featurization function"
    }
   },
   "outputs": [],
   "source": [
    "def clean_churn_features(dataDF: SparkDataFrame) -> SparkDataFrame:\n",
    "  \"\"\"\n",
    "  Simple cleaning function leveraging pandas API\n",
    "  \"\"\"\n",
    "\n",
    "  # Convert to pandas on spark dataframe\n",
    "  data_psdf = dataDF.pandas_api()\n",
    "\n",
    "  # Convert some columns\n",
    "  data_psdf = data_psdf.astype({\"senior_citizen\": \"string\"})\n",
    "  data_psdf[\"senior_citizen\"] = data_psdf[\"senior_citizen\"].map({\"1\" : \"Yes\", \"0\" : \"No\"})\n",
    "\n",
    "  data_psdf[\"total_charges\"] = data_psdf[\"total_charges\"].apply(lambda x: float(x) if x.strip() else 0)\n",
    "\n",
    "  # Fill some missing numerical values with 0\n",
    "  data_psdf = data_psdf.fillna({\"tenure\": 0.0})\n",
    "  data_psdf = data_psdf.fillna({\"monthly_charges\": 0.0})\n",
    "  data_psdf = data_psdf.fillna({\"total_charges\": 0.0})\n",
    "\n",
    "  # Add/Force semantic data types for specific colums (to facilitate autoML)\n",
    "  data_cleanDF = data_psdf.to_spark()\n",
    "  data_cleanDF = data_cleanDF.withMetadata(\"customer_id\", {\"spark.contentAnnotation.semanticType\":\"native\"})\n",
    "  data_cleanDF = data_cleanDF.withMetadata(\"num_optional_services\", {\"spark.contentAnnotation.semanticType\":\"numeric\"})\n",
    "\n",
    "  return data_cleanDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e3aa934-fe34-496f-8cb1-c2628cde77ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Compute & Write to Feature Store\n",
    "\n",
    "Once our features are ready, we'll save them in Databricks Feature Store. Any Delta Table registered to Unity Catalog can be used as a feature table.\n",
    "\n",
    "This will allows us to leverage Unity Catalog for governance, discoverability and reusability of our features accross our organization, as well as increasing team efficiency.\n",
    "\n",
    "The lineage capability in Unity Catalog brings traceability and governance in our deployment, knowing which model is dependent of which feature tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64723754-715a-442a-bf15-61ebde561bea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute Churn Features and append a timestamp"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "# Add current scoring timestamp\n",
    "this_time = (datetime.now()).timestamp()\n",
    "churn_features_n_predsDF = clean_churn_features(compute_service_features(telcoDF)) \\\n",
    "                            .withColumn(\"transaction_ts\", lit(this_time).cast(\"timestamp\"))\n",
    "\n",
    "display(churn_features_n_predsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f5a929-c7a5-46ff-925e-6119a2502b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Extract ground-truth labels in a separate table to avoid label leakage\n",
    "* In reality ground-truth label data should be in its own separate table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36d08498-559f-4da5-be65-c74a5ba45f1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract ground-truth labels in a separate table and drop from Feature table"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "# Best practice: specify train-val-test split as categorical label (to be used by automl and/or model validation jobs)\n",
    "train_ratio, val_ratio, test_ratio = 0.7, 0.2, 0.1\n",
    "\n",
    "churn_features_n_predsDF.select(\"customer_id\", \"transaction_ts\", \"churn\") \\\n",
    "                        .withColumn(\"random\", F.rand(seed=42)) \\\n",
    "                        .withColumn(\"split\",\n",
    "                                    F.when(F.col(\"random\") < train_ratio, \"train\")\n",
    "                                    .when(F.col(\"random\") < train_ratio + val_ratio, \"validate\")\n",
    "                                    .otherwise(\"test\")) \\\n",
    "                        .drop(\"random\") \\\n",
    "                        .write.format(\"delta\") \\\n",
    "                        .mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \\\n",
    "                        .saveAsTable(f\"advanced_churn_label_table\")\n",
    "\n",
    "churn_featuresDF = churn_features_n_predsDF.drop(\"churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce552085-d4d2-4873-930c-b477e3f94da2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Add primary keys constraints to labels table for feature lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a860fd7-de9a-46af-ac7c-d3bd6b00a0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE advanced_churn_label_table ALTER COLUMN customer_id SET NOT NULL;\n",
    "ALTER TABLE advanced_churn_label_table ALTER COLUMN transaction_ts SET NOT NULL;\n",
    "ALTER TABLE advanced_churn_label_table ADD CONSTRAINT advanced_churn_label_table_pk PRIMARY KEY(customer_id, transaction_ts);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9f82a0c-9ba1-4e46-8bb3-b942a062a6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write the feature table to Unity Catalog\n",
    "\n",
    "With Unity Catalog, any Delta table with a primary key constraint can be used as a offline feature table.\n",
    "\n",
    "Time series feature tables have an additional primary key on the time column.\n",
    "\n",
    "After the table is created, you can write data to it like other Delta tables, and use it as a feature table.\n",
    "\n",
    "Here, we demonstrate creating the feature table using the `FeatureEngineeringClient` API. You can also easily create it using SQL:\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "CREATE TABLE {catalog}.{db}.{feature_table_name} (\n",
    "  {primary_key} int NOT NULL,\n",
    "  {timestamp_col} timestamp NOT NULL,\n",
    "  feat1 long,\n",
    "  feat2 varchar(100),\n",
    "  CONSTRAINT customer_features_pk PRIMARY KEY ({primary_key}, {timestamp_col} TIMESERIES)\n",
    ");\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26542be1-14a2-4ba2-abc6-8735953e661e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "First, since we are creating the feature table from scratch, we want to make sure that our environment is clean and any previously created offline/online feature tables are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e773277b-ab69-45e3-aa20-0220366b819c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop any existing online table (optional)"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "\n",
    "# Create workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Remove any existing online feature table\n",
    "try:\n",
    "  online_table_specs = w.online_tables.get(f\"{catalog}.{db}.advanced_churn_feature_table_online_table\")\n",
    "  # Drop existing online feature table\n",
    "  w.online_tables.delete(f\"{catalog}.{db}.advanced_churn_feature_table_online_table\")\n",
    "  print(f\"Dropping online feature table: {catalog}.{db}.advanced_churn_feature_table_online_table\")\n",
    "\n",
    "except Exception as e:\n",
    "  pprint(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aaa6f5e-8a77-462a-8a3c-b84935abc496",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop feature table if it already exists"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- We are creating the feature table from scratch.\n",
    "-- Let's drop any existing feature table if it exists\n",
    "DROP TABLE IF EXISTS advanced_churn_feature_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35d4865c-d091-45f2-9d4f-61e99609507d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Feature Store Client"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "\n",
    "fe = FeatureEngineeringClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0230300-b8cd-446c-b9f4-fe0cf048e27e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create \"feature\"/UC table"
    }
   },
   "outputs": [],
   "source": [
    "churn_feature_table = fe.create_table(\n",
    "  name=\"advanced_churn_feature_table\", # f\"{catalog}.{dbName}.{feature_table_name}\"\n",
    "  primary_keys=[\"customer_id\", \"transaction_ts\"],\n",
    "  schema=churn_featuresDF.schema,\n",
    "  timeseries_columns=\"transaction_ts\",\n",
    "  description=f\"These features are derived from the {catalog}.{db}.{bronze_table_name} table in the lakehouse. We created service features, cleaned up their names.  No aggregations were performed. [Warning: This table doesn't store the ground-truth and now can be used with AutoML's Feature Store integration\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f709bb5-8268-48bc-ae56-952b0724eae0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write feature values to Feature Store"
    }
   },
   "outputs": [],
   "source": [
    "fe.write_table(\n",
    "  name=f\"{catalog}.{db}.advanced_churn_feature_table\",\n",
    "  df=churn_featuresDF, # can be a streaming dataframe as well\n",
    "  mode='merge' #'merge' supports schema evolution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed40327b-982c-4045-bb1d-c0ed0fc6a6de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Featurization Logic for on-demand feature functions\n",
    "\n",
    "We will define a function for features that need to be calculated on-demand. These functions can be used in both batch/offline and serving/online inference.\n",
    "\n",
    "It is common that customers who have elevated bills of monthly charges have a higher propensity to churn. The `avg_price_increase` function calculates the potential average price increase based on their historical charges, as well as their current tenure. The function lets the model use this freshly calculated value as a feature for training and, later, scoring.\n",
    "\n",
    "This function is defined under Unity Catalog, which provides governance over who can use the function.\n",
    "\n",
    "Refer to the documentation for more information. ([AWS](https://docs.databricks.com/en/machine-learning/feature-store/on-demand-features.html)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/feature-store/on-demand-features)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "310975b0-c541-4db2-bfd0-aba32ed0c972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "  CREATE OR REPLACE FUNCTION avg_price_increase(monthly_charges_in DOUBLE, tenure_in DOUBLE, total_charges_in DOUBLE)\n",
    "  RETURNS FLOAT\n",
    "  LANGUAGE PYTHON\n",
    "  COMMENT \"[Feature Function] Calculate potential average price increase for tenured customers based on last monthly charges and updated tenure\"\n",
    "  AS $$\n",
    "  if tenure_in > 0:\n",
    "    return monthly_charges_in - total_charges_in/tenure_in\n",
    "  else:\n",
    "    return 0\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e9c0848-aec9-419a-b313-73a72c848a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE FUNCTION avg_price_increase;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f5cdb48-f060-4d9f-9f85-445bef0d1b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Accelerating Churn model creation using Databricks Auto-ML\n",
    "### A glass-box solution that empowers data teams without taking away control\n",
    "\n",
    "Databricks simplify model creation and MLOps. However, bootstraping new ML projects can still be long and inefficient.\n",
    "\n",
    "Instead of creating the same boilerplate for each new project, Databricks Auto-ML can automatically generate state of the art models for Classifications, regression, and forecast.\n",
    "\n",
    "Models can be directly deployed, or instead leverage generated notebooks to boostrap projects with best-practices, saving you weeks of efforts.\n",
    "\n",
    "<img width=\"1000\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/auto-ml-full.png\"/>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Using Databricks Auto ML with our Churn dataset\n",
    "\n",
    "<br>\n",
    "\n",
    "<img style=\"float: right\" width=\"600\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/churn-auto-ml.png\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "Auto ML is available under **Machine Learning - Experiments**. All we have to do is create a new Auto-ML experiment and select the table containing the ground-truth labels and join it with the features in the feature table.\n",
    "\n",
    "Our prediction target is the `churn` column.\n",
    "\n",
    "Click on **Start**, and Databricks will do the rest.\n",
    "\n",
    "While this is done using the UI, you can also leverage the [python API](https://docs.databricks.com/applications/machine-learning/automl.html#automl-python-api-1)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Join/Use features directly from the Feature Store from the [UI](https://docs.databricks.com/machine-learning/automl/train-ml-model-automl-ui.html#use-existing-feature-tables-from-databricks-feature-store) or [python API]()\n",
    "* Select the table containing the ground-truth labels (i.e. `dbdemos.schema.churn_label_table`)\n",
    "* Join remaining features from the feature table (i.e. `dbdemos.schema.churn_feature_table`)\n",
    "\n",
    "Refer to the __Quickstart__ version of this demo for an example of AutoML in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "801b9303-4294-40ae-8569-b2db0aa0fb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using the AutoML-generated notebook to build our model\n",
    "\n",
    "We have pre-run AutoML, which generated the notebook that trained the best model in the AutoML run. We take this notebook and improve on the model.\n",
    "\n",
    "Next step: [Explore the modfied version of the notebook generated from Auto-ML]($./02_automl_champion)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
