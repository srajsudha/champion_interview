{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eb77950-b9bf-4b2a-8b56-ee26a9ac4a75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Monitor Model using Lakehouse Monitoring\n",
    "In this step, we will leverage Databricks Lakehouse Monitoring([AWS](https://docs.databricks.com/en/lakehouse-monitoring/index.html)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/lakehouse-monitoring/)) to monitor our inference table.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/advanced/banners/mlflow-uc-end-to-end-advanced-7.png?raw=true\" width=\"1200\">\n",
    "\n",
    "Databricks Lakehouse Monitoring attaches a data monitor to any Delta table and it will generate the necessary pipelines to profile the data and calculate quality metrics. You just need to tell it how frequently these quality metrics need to be collected.\n",
    "\n",
    "Use Databricks Lakehouse Monitoring to monitor for data drifts, as well as label drift, prediction drift and changes in model quality metrics in Machine Learning use cases. Databricks Lakehouse Monitoring enables monitoring for statistics (e.g. data profiles) and drifts on tables containing:\n",
    "* batch scoring inferences\n",
    "* request logs from Model Serving endpoint ([AWS](https://docs.databricks.com/en/machine-learning/model-serving/inference-tables.html) |[Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/inference-tables))\n",
    "\n",
    "Databricks Lakehouse Monitoring stores the data quality and drift metrics in two tables that it automatically creates for each monitored table:\n",
    "- Profile metrics table (with a `_profile_metrics` suffix)\n",
    "  - Metrics like percentage of null values, descriptive statistics, model metrics such as accuracy, RMSE, fairness and bias metrics etc.\n",
    "- Drift metrics table (with a `_drift_metrics` suffix)\n",
    "  - Metrics like the \"delta\" between percentage of null values, averages, as well as metrics from statistical tests to detect data drift.\n",
    "\n",
    "For demo simplicity purpose, we will use the batch scoring model inference as our inference table. We will attach a monitor to the table `mlops_churn_advanced_inference_table`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87416dbc-3f6b-4942-8627-044f65b93ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": null
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `Current Cluster` from the dropdown menu ([open cluster configuration](https://enb-deloitte.cloud.databricks.com/#setting/clusters/0106-134225-g5g75oj5/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('mlops-end2end')` or re-install the demo: `dbdemos.install('mlops-end2end')`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a412f4cf-880c-4f36-9681-80ac657cf28a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install latest databricks-sdk package (>=0.28.0)"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qU databricks-sdk==0.40.0 mlflow==2.19\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a231043-98a7-48ce-9184-fb7d108f5fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false $adv_mlops=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d71fc42-8156-45c0-9edd-8ca808710be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create monitor\n",
    "Now, we will create a monitor on top of the inference table. \n",
    "It is a one-time setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a5eacb3-2ce5-40d5-b71b-6b6341e91360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Inference Table\n",
    "\n",
    "This can serve as a union for offline & online processed inference.\n",
    "For simplicity of this demo, we will create the inference table as a copy of the first offline batch prediction table.\n",
    "\n",
    "In a different scenario, we could have processed the online inference table and store them in the inference table alongside with the offline inference table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f26ba03-cc73-4560-8577-4e13a45adfcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE advanced_churn_inference_table AS\n",
    "          SELECT * EXCEPT (split) FROM advanced_churn_offline_inference LEFT JOIN advanced_churn_label_table USING(customer_id, transaction_ts) ;\n",
    "\n",
    "ALTER TABLE advanced_churn_inference_table SET TBLPROPERTIES (delta.enableChangeDataFeed = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56b7622e-318f-4441-b36f-70c0b9fd7aca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create baseline table\n",
    "\n",
    "For simplification purposes, we will create the baseline table from the pre-existing `advanced_churn_offline_inference` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f34a751-8ca3-4f1d-8ff9-ed34462c9495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO: understand why we need model version in the baseline table\n",
    "CREATE OR REPLACE TABLE advanced_churn_baseline AS\n",
    "  SELECT * EXCEPT (customer_id, transaction_ts, model_alias, inference_timestamp) FROM advanced_churn_inference_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3c11c27-248d-4198-84b3-225ccde768e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a custom metric\n",
    "\n",
    "Customer metrics can be defined and will automatically be calculated by lakehouse monitoring. They often serve as a mean to capture some aspect of business logic or use a custom model quality score. \n",
    "\n",
    "In this example, we will calculate the business impact (loss in monthly charges) of a bad model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b87f8fc7-ee1f-40e3-bbe7-4657b477442d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define expected loss metric"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StructField\n",
    "from databricks.sdk.service.catalog import MonitorMetric, MonitorMetricType\n",
    "\n",
    "expected_loss_metric = [\n",
    "  MonitorMetric(\n",
    "    type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,\n",
    "    name=\"expected_loss\",\n",
    "    input_columns=[\":table\"],\n",
    "    definition=\"\"\"avg(CASE\n",
    "    WHEN {{prediction_col}} != {{label_col}} AND {{label_col}} = 'Yes' THEN -monthly_charges\n",
    "    ELSE 0 END\n",
    "    )\"\"\",\n",
    "    output_data_type= StructField(\"output\", DoubleType()).json()\n",
    "  )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb9aa492-f4ab-4ba6-8ba1-16987fa8a8b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create monitor\n",
    "\n",
    "As we are monitoring an inference table (including machine learning model predcitions data), we will pick an [Inference profile](https://learn.microsoft.com/en-us/azure/databricks/lakehouse-monitoring/create-monitor-api#inferencelog-profile) for the monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85319cfb-620b-483e-95f9-d4adc3015a50",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Monitor"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import MonitorInferenceLog, MonitorInferenceLogProblemType\n",
    "\n",
    "print(f\"Creating monitor for inference table {catalog}.{db}.advanced_churn_inference_table\")\n",
    "w = WorkspaceClient()\n",
    "\n",
    "try:\n",
    "  info = w.quality_monitors.create(\n",
    "    table_name=f\"{catalog}.{db}.advanced_churn_inference_table\",\n",
    "    inference_log=MonitorInferenceLog(\n",
    "            problem_type=MonitorInferenceLogProblemType.PROBLEM_TYPE_CLASSIFICATION,\n",
    "            prediction_col=\"prediction\",\n",
    "            timestamp_col=\"inference_timestamp\",\n",
    "            granularities=[\"1 day\"],\n",
    "            model_id_col=\"model_version\",\n",
    "            label_col=\"churn\", # optional\n",
    "    ),\n",
    "    assets_dir=f\"{os.getcwd()}/monitoring\", # Change this to another folder of choice if needed\n",
    "    output_schema_name=f\"{catalog}.{db}\",\n",
    "    baseline_table_name=f\"{catalog}.{db}.advanced_churn_baseline\",\n",
    "    slicing_exprs=[\"senior_citizen='Yes'\", \"contract\"], # Slicing dimension\n",
    "    custom_metrics=expected_loss_metric)\n",
    "  \n",
    "except Exception as lhm_exception:\n",
    "  if \"already exist\" in str(lhm_exception):\n",
    "    print(f\"Monitor for {catalog}.{db}.advanced_churn_inference_table already exists, retrieving monitor info:\")\n",
    "    info = w.quality_monitors.get(table_name=f\"{catalog}.{db}.advanced_churn_inference_table\")\n",
    "  else:\n",
    "    raise lhm_exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4dd2991-ccdc-4e54-8fcf-8db15ff8cad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wait/Verify that monitor was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cd35a86-253d-4f22-84ca-3f7fb43dfe9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from databricks.sdk.service.catalog import MonitorInfoStatus, MonitorRefreshInfoState\n",
    "\n",
    "# Wait for monitor to be created\n",
    "while info.status == MonitorInfoStatus.MONITOR_STATUS_PENDING:\n",
    "  info = w.quality_monitors.get(table_name=f\"{catalog}.{db}.advanced_churn_inference_table\")\n",
    "  time.sleep(10)\n",
    "\n",
    "assert info.status == MonitorInfoStatus.MONITOR_STATUS_ACTIVE, \"Error creating monitor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb3edf28-8a7d-4817-8ef5-d89ca6089dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Monitor creation for the first time will also **trigger an initial refresh** so fetch/wait or trigger a monitoring job and wait until completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f01bc80-95f2-4daf-9741-3c4f2d71851f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_refreshes():\n",
    "  return w.quality_monitors.list_refreshes(table_name=f\"{catalog}.{db}.advanced_churn_inference_table\").refreshes\n",
    "\n",
    "refreshes = get_refreshes()\n",
    "if len(refreshes) == 0:\n",
    "  w.quality_monitors.run_refresh(table_name=f\"{catalog}.{db}.advanced_churn_inference_table\")\n",
    "  time.sleep(5)\n",
    "  refreshes = get_refreshes()\n",
    "\n",
    "run_info = refreshes[0]\n",
    "while run_info.state in (MonitorRefreshInfoState.PENDING, MonitorRefreshInfoState.RUNNING):\n",
    "  run_info = w.quality_monitors.get_refresh(table_name=f\"{catalog}.{db}.advanced_churn_inference_table\", refresh_id=run_info.refresh_id)\n",
    "  print(f\"waiting for refresh to complete {run_info.state}...\")\n",
    "  time.sleep(30)\n",
    "\n",
    "assert run_info.state == MonitorRefreshInfoState.SUCCESS, \"Monitor refresh failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2956d5e-54c9-447a-a703-2b2215b703d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w.quality_monitors.get(table_name=f\"{catalog}.{db}.advanced_churn_inference_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b980ba8-afa0-46af-a01e-afd8ac9751ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Delete existing monitor [OPTIONAL]"
    }
   },
   "outputs": [],
   "source": [
    "# w.quality_monitors.delete(table_name=f\"{catalog}.{db}.advanced_churn_offline_inference\", purge_artifacts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "605099a6-af3e-41f7-8b45-f44244817342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inspect dashboard\n",
    "\n",
    "You can now inspect the monitoring dashboard that is automatically generated for you. Navigate to `advanced_churn_inference_table` in the __Catalog Explorer__, go to the __Quality__ tab and click on the __View dashboard__ button.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/advanced/07_view_dashboard_button.png?raw=true\" width=\"480\">\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "You can see the number of inferences being done before the first monitor refresh (the first refresh \"window\"), as well as the model performance metrics.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/advanced/07_model_inferences.png?raw=true\" width=\"1200\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Scrolling further down to the section on __Prediction drift__, you can see the confusion matrix and the percentage of the model's predictions.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/advanced/07_confusion_matrix.png?raw=true\" width=\"1200\">\n",
    "\n",
    "<br>\n",
    "\n",
    "We do not observe any drift yet, as we only have the first refresh \"window\". We will simulate some drifted data in the next step and refresh the monitor against the newly captured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4367751-d5bf-47b4-8437-2d2657bd6c77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Next: Test for drift and trigger a model retrain\n",
    "\n",
    "Now, let explore how to detect drift on the inference data and define violations rules for triggering a model (re)train workflow.\n",
    "\n",
    "Next steps:\n",
    "* [Detect drift and trigger model retrain]($./08_drift_detection)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "07_model_monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
