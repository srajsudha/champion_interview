{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40d7b8f8-84e8-48ed-a0f0-bb77c31a2da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./utlis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0896b56-bcc9-4da2-b389-a18d041c86d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5b4ffa-88ac-4875-a3a0-70fa592642a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_last_timestamp(watermark_table):\n",
    "    try:\n",
    "        # Read the last read timestamp from the watermark table\n",
    "        watermark_df = spark.read.table(watermark_table)\n",
    "        # Get the last read timestamp\n",
    "        last_read_timestamp = watermark_df.select(\"last_read_timestamp\").collect()[0][0]\n",
    "        return last_read_timestamp\n",
    "    except AnalysisException as e:\n",
    "        logger.error(f\"Error reading watermark table: {e}\")\n",
    "        raise\n",
    "    except IndexError:\n",
    "        logger.warning(\"Watermark table is empty. Setting last_read_timestamp to '1970-01-01 00:00:00'\")\n",
    "        return '1970-01-01 00:00:00'\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fab2f6b-fa87-4754-8f03-104806211e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_new_data(jdbcUrl, jdbcUsername, jdbcPassword, source_table, last_read_timestamp):\n",
    "    try:\n",
    "        # Read new data from the source table using the last read timestamp\n",
    "        new_data_df = (spark.read\n",
    "                       .format(\"jdbc\")\n",
    "                       .option(\"url\", jdbcUrl)\n",
    "                       .option(\"user\", jdbcUsername)\n",
    "                       .option(\"password\", jdbcPassword)\n",
    "                       .option(\"query\", f\"SELECT * FROM {source_table} WHERE last_updated > '{last_read_timestamp}'\")\n",
    "                       .load())\n",
    "        return new_data_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading new data from source table: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e8f358-b4a9-4f1a-9ad1-d6beeaac1ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_watermark_table(watermark_table, latest_timestamp):\n",
    "    try:\n",
    "        # Update the watermark table with the latest timestamp\n",
    "        if latest_timestamp:\n",
    "            spark.sql(f\"UPDATE {watermark_table} SET last_read_timestamp = '{latest_timestamp}'\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating watermark table: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b97fd84d-7098-4939-869a-08cb43f097ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_incremental_data(source_table):\n",
    "    try:\n",
    "        # Read incremental data from the source table\n",
    "        incremental_data_df = (spark.readStream\n",
    "                               .format(\"delta\")\n",
    "                               .table(source_table))\n",
    "        logger.info(f\"Started reading from {source_table}\")\n",
    "        return incremental_data_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading from {source_table}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4302ea-a49a-4dc8-8b27-50685bfe4171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_delta_data(source_table):\n",
    "    try:\n",
    "        # Read incremental data from the source table\n",
    "        incremental_data_df = (spark.read\n",
    "                               .format(\"delta\")\n",
    "                               .table(source_table))\n",
    "        logger.info(f\"Started reading from {source_table}\")\n",
    "        return incremental_data_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading from {source_table}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30e92bb9-0bf4-44f9-a66f-85f584903345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_data(data_df, target_table):\n",
    "    try:\n",
    "        # Write the data to the target table\n",
    "        (data_df.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"append\")\n",
    "         .saveAsTable(target_table))\n",
    "        logger.info(f\"Started writing to {target_table}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in writing to {target_table}: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "common",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
